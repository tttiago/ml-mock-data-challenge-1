{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31677b61",
   "metadata": {},
   "source": [
    "\n",
    "# Machine learning tutorial\n",
    "# MLGWSC-1 kickoff meeting\n",
    "### 12. 10. 2021, Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bb8fe",
   "metadata": {},
   "source": [
    "## What is ML?\n",
    "**Tom M. Mitchell**: *A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ed990",
   "metadata": {},
   "source": [
    "## Basic formulation of ML problem\n",
    "We need to approximate function $f^\\star : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m,~ f^\\star\\left(\\mathbf{x}\\right) = \\mathbf{y}$.\n",
    "\n",
    "Let us define a *model* $f : \\mathbb{R}^n \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^m,~ f\\left(\\mathbf{x}, \\theta\\right) = \\mathbf{y}$; typically: $\\mathbf{x}$ = inputs, $\\mathbf{y}$ = outputs, $\\theta$ = weights.\n",
    "\n",
    "If the model is suitable to the problem, then a set of weights $\\hat{\\theta}$ exists such that $f\\left(\\mathbf{x}, \\hat{\\theta}\\right) \\approx f^\\star\\left(\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fca1a8",
   "metadata": {},
   "source": [
    "### How to find the right weights?\n",
    "\n",
    "Two more necessary ingredients:\n",
    "\n",
    "Dataset: $\\mathbf{X}, \\mathbf{Y}, \\forall i: \\mathbf{Y}_i = f^\\star\\left(\\mathbf{X}_i\\right) + \\mathrm{noise}$, $\\mathbf{Y}$ are called labels\n",
    "\n",
    "Loss/cost/error function: $\\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)$ to measure deviation of the model from the labels (e.g. mean squared error)\n",
    "\n",
    "Training the model: $\\hat{\\theta} = \\mathrm{argmin}_\\theta\\{\\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)\\} \\rightarrow \\boxed{f\\left(\\mathbf{x}, \\hat{\\theta}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa25c2",
   "metadata": {},
   "source": [
    "## Example: Polynomial regression\n",
    "\n",
    "We'll attempt to approximate the polynomial $f^\\star\\left(x\\right) = 2x - 10x^5 + 15x^{10}$ (*true model*) using polynomial regression: minimization of the mean squared error $\\mathcal{C}_{MSE}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\left(\\mathbf{Y}_i - f\\left(\\mathbf{X}_i, \\theta\\right)\\right)^2$, and the weights are the coefficients of the polynomial.\n",
    "\n",
    "If the $\\mathbf{X}$ inputs are non-degenerate (no two are the same) and there are more than the polynomial degree, there is a single minimum and it is easy to compute.\n",
    "\n",
    "We'll sample some values of $f^\\star$ with a little white Gaussian noise as a **training dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc079367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "### Create the true model and some values to plot\n",
    "poly = np.polynomial.Polynomial([0., 2., 0., 0., 0., -10., 0., 0., 0., 0., 15.])\n",
    "plot_X = np.linspace(0., 1., 10**3)\n",
    "plot_Y = poly(plot_X)\n",
    "### Create the plot limits\n",
    "xlim = (0., 1.)\n",
    "ylim = (min(plot_Y), max(plot_Y))\n",
    "extend_factor = .1\n",
    "xlim = (xlim[0]-extend_factor*(xlim[1]-xlim[0]), xlim[1]+extend_factor*(xlim[1]-xlim[0]))\n",
    "ylim = (ylim[0]-extend_factor*(ylim[1]-ylim[0]), ylim[1]+extend_factor*(ylim[1]-ylim[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f123193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Generate training data\n",
    "# tr_X = rng.uniform(0., 1., 11)\n",
    "tr_X = np.linspace(0., 1., 11)\n",
    "tr_Y = poly(tr_X) + rng.normal(0., .3, len(tr_X))\n",
    "\n",
    "### Plot the true model and the training data\n",
    "plt.figure(figsize=(10., 5.))\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.plot(plot_X, plot_Y, label='true model')\n",
    "plt.scatter(tr_X, tr_Y, label='training data')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab099f7",
   "metadata": {},
   "source": [
    "Now, let's train some polynomials. Change the line that starts with `deg` to set the orders of fitting polynomials. One might expect that since we have the 11 points required to fit a 10-degree polynomial and the data was generated by a 10-degree polynomial, that would again reproduce the true model the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit polynomials\n",
    "degs = [1, 3, 5, 10]\n",
    "fit_polys = [poly.fit(tr_X, tr_Y, deg) for deg in degs]\n",
    "\n",
    "### Plot the fitted polynomials\n",
    "plt.figure(figsize=(15., 10.))\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "for deg, fit_poly in zip(degs, fit_polys):\n",
    "    plt.plot(plot_X, fit_poly(plot_X), label='%i'%deg)\n",
    "plt.scatter(tr_X, tr_Y, label='training data')\n",
    "plt.plot(plot_X, plot_Y, label='true model')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1c659",
   "metadata": {},
   "source": [
    "Generate some **test data** and evaluate the training (*in-sample*) and test (*out-of-sample*) losses of a set of polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit polynomials\n",
    "degs = range(11)\n",
    "fit_polys = [poly.fit(tr_X, tr_Y, deg) for deg in degs]\n",
    "\n",
    "### Generate test data, same distribution as training data\n",
    "te_X = rng.uniform(0., 1., 100)\n",
    "te_Y = poly(te_X) + rng.normal(0., .3, len(te_X))\n",
    "\n",
    "### Compute the loss values\n",
    "losses = []\n",
    "for deg, fit_poly in zip(degs, fit_polys):\n",
    "    new_losses = []\n",
    "    for X, Y in ((tr_X, tr_Y), (te_X, te_Y)):\n",
    "        model_Y = fit_poly(X)\n",
    "        new_losses.append(np.mean((model_Y - Y)**2))\n",
    "    losses.append(new_losses)\n",
    "losses = np.array(losses)\n",
    "\n",
    "### Plot the loss values\n",
    "plt.figure(figsize=(10., 5.))\n",
    "plt.plot(degs, losses[:, 0], '.-', label='training')\n",
    "plt.plot(degs, losses[:, 1], '.-', label='test')\n",
    "plt.semilogy()\n",
    "plt.ylim((1.e-2, 1.e1))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('polynomial degree')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e34fc",
   "metadata": {},
   "source": [
    "In fact, the degree-10 polynomial is not a good fit. The plot above demonstrates that with a limited number of noisy data points, too high complexity model gives the best training loss (so it's a very good fit to that particular set of data), but has trouble generalizing to new data from the same underlying distribution. This phenomenon is called **overfitting**. It is the difference between fitting and predicting.\n",
    "\n",
    "One must therefore, when working with more complex data and models, be careful about model complexity - increasing it may lead to a better fit on training data and at the same time, to worse predictions on unseen data. Even if a model is perfectly suited to a given underlying true model, in the presence of limited and/or noisy data, a less complex model may reproduce the true model better. The only way to prevent this issue is to keep an eye on the out-of-sample loss. This is achieved by a dataset split.\n",
    "\n",
    "In some cases, a dataset is supplied already split - in other cases, one must split the data themselves. In a lot of cases where the optimization is iterative, one must have a third separate dataset, called the **validation set**. Upon each iteration of the optimization algorithm over the training set, the loss on the validation set is evaluated to make sure that it is decreasing, and the training is typically interrupted once this is no longer the case. This in turn might also lead to slight overfitting on the validation dataset, which is why we still need the test set despite already having computed an out-of-sample validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d34aa",
   "metadata": {},
   "source": [
    "## Gradient descent algorithms\n",
    "\n",
    "Polynomial regression was a simple example, which is easy to solve - the loss optimization has a single global minimum (under weak assumptions). With more complex ML models, this is no longer the case. Usually, gradient descent-based algorithms are used to find the minimum of $E\\left(\\theta\\right) = \\mathcal{C}\\left(\\mathbf{Y}, f\\left(\\mathbf{X}, \\theta\\right)\\right)$. These algorithms require an \"initial position\" $\\theta_0$ and attempt to find the solution iteratively.\n",
    "\n",
    "Basic gradient descent:\n",
    "$$\\mathbf{v}_t = \\eta\\nabla_\\theta E\\left(\\theta_t\\right),~\\theta_{t+1} = \\theta_t - \\mathbf{v}_t$$\n",
    "\n",
    "Let's try on a simple parabolic surface $E\\left(\\theta\\right) = \\theta_0^2 + \\theta_1^2$. Feel free to play around with the value of `eta` to see why some choices of learning rate are unsuitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parabolic surface and its gradient\n",
    "def surface(theta):\n",
    "    return np.sum(theta**2, axis=0)\n",
    "\n",
    "def gradient(theta):\n",
    "    return 2*theta\n",
    "\n",
    "### Generate contour plot data\n",
    "X = np.linspace(-5., 5., 1000)\n",
    "Y = np.linspace(-5., 5., 1000)\n",
    "Z = surface(np.stack((np.tile(np.expand_dims(X, 1), (1, len(Y))), np.tile(np.expand_dims(Y, 0), (len(X), 1))), axis=0)).T\n",
    "\n",
    "### Initial position and GD parameters\n",
    "theta = [np.array((-4., -2.))]\n",
    "epochs = range(21)\n",
    "eta = 0.5\n",
    "\n",
    "### GD loop\n",
    "for e in epochs[1:]:\n",
    "    v = eta*gradient(theta[-1])\n",
    "    theta.append(theta[-1]-v)\n",
    "theta = np.stack(theta, axis=1)\n",
    "\n",
    "### Plot results\n",
    "plt.figure(figsize=(7., 7.))\n",
    "plt.contour(X, Y, Z)\n",
    "plt.plot(theta[0], theta[1], '.-', linewidth=1., markersize=10.)\n",
    "plt.axis('square')\n",
    "plt.xlim((min(X), max(X)))\n",
    "plt.ylim((min(Y), max(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef48c0",
   "metadata": {},
   "source": [
    "Usually, the landscape which we need our optimizer to navigate will be much more rugged with local minima and saddle points - and much higher-dimensional, so impossible to visualize! This makes it much harder to determine a suitable learning rate. However, smart adaptive algorithms exist which keep slowly decaying running averages of the gradients and other properties, which allows them to locally estimate the Hessian matrix and effectively adapt the learning rate to its ideal value. We won't learn about these in this tutorial - to try them out, simply replace the `torch.optim.SGD` by, e.g., `torch.optim.Adam`.\n",
    "\n",
    "To be able to work with complex data, we need complex models - so we need the datasets to be large. Computation of the gradient of $E\\left(\\theta\\right)$ over the full training dataset is then computationally very difficult, let alone doing several tens or hundreds of optimization steps, so we need to apply one more approximation to our method.\n",
    "\n",
    "We assume that evaluating the loss gradient over a smaller subset of the training dataset is representative of the full computation. We split the training dataset into **batches** and do a single step of the gradient descent method on each of these batches. One iteration over all the batches is called a **training epoch**. This method is called **stochastic gradient descent**. The same principle is also used with the adaptive optimizers mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60357337",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are inspired by the human neural system. The basic element is a very simple **artificial neuron**, which merely takes a linear combination of its input values, with an additional constant term, and applies a non-linear function:\n",
    "$$y(\\mathbf{x}) = \\sigma\\left(\\sum_{i} w_ix_i + b\\right).$$\n",
    "\n",
    "The $w_i$ values are usually called **weights** and $b$ is the **bias**. The function $\\sigma$ is called the **activation function** and there are many different choices. Some of the most popular are, for example:\n",
    "$$\\mathrm{Sigmoid}\\left(x\\right) = \\frac{1}{1+e^{-x}},~\\mathrm{ReLU}\\left(x\\right) = \\mathrm{max}\\left(0, x\\right)~.$$\n",
    "\n",
    "For classification problems, the *Softmax* is very popular, because it maps $\\mathbb{R}^N$ to a set of positive numbers that sum up to one. This is often used as a final activation to produce approximate probabilities as a model output (in other places in the network, however, others are typically used).\n",
    "$$\\mathrm{Softmax}\\left(\\mathbf{x}\\right)_i = \\frac{\\exp\\left(x_i\\right)}{\\sum_j \\exp\\left(x_j\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e31ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the Softmax and ReLU activation functions\n",
    "X = np.linspace(-10., 10., 10000)\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10., 5.))\n",
    "\n",
    "Y = 1./(1.+np.exp(-X))\n",
    "axes[0].plot(X, Y, label='Sigmoid')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "Y = np.amax(np.stack((X, np.zeros_like(X)), axis=0), axis=0)\n",
    "axes[1].plot(X, Y, label='ReLU')\n",
    "axes[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e15d2",
   "metadata": {},
   "source": [
    "We can order multiple independent neurons into a **layer**, so that:\n",
    "$$y_i\\left(\\mathbf{x}\\right) = \\sigma\\left(\\sum_j w_{ij}x_j + b_i\\right).$$\n",
    "\n",
    "Let us apply this to a real-life classification problem. We will use the MNIST dataset, which contains 60 000 annotated hand-written digits as single-channel $28\\times 28$ images and 10 000 more for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "fname = 'MNIST'\n",
    "\n",
    "### Download and initialize datasets\n",
    "TrainDS_orig = torchvision.datasets.MNIST(fname, train=True, download=True)\n",
    "TestDS_orig = torchvision.datasets.MNIST(fname, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4707fe0",
   "metadata": {},
   "source": [
    "Plot a few examples of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9540119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot examples\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        test_index = rng.integers(0, len(TestDS_orig))\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i' % orig_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080780a",
   "metadata": {},
   "source": [
    "Let's try to use a simple neuron layer to classify the digits in the MNIST dataset. We first need to create a function that will transform an integer label to a set of probabilities, e.g., $1 \\to (0., 1., 0., ..., 0.)$: $i$-th element represents the probability that the sample contains the digit $i$. We reinitialize the datasets using this transform as well as one which turns our input images into $28\\times 28$ PyTorch tensors.\n",
    "\n",
    "Then we define the model itself as a sequence of functions. In the PyTorch vernacular, the first is `torch.nn.Flatten`; it rearranges the input $28\\times 28$ tensor into a vector of length $28\\cdot 28 = 784$. The next `torch.nn.Linear` is the linear combination performed by the neurons: this outputs 10 numbers, each of which is a linear combination of the 784 inputs with independent coefficients as well as constant biases, all of which are trainable weights. Finally, the `torch.nn.Softmax` performs the Softmax operation as described a few blocks above. We will use the SGD method with learning rate 0.1 and batch size 32 and as a loss function, we will use the binary cross entropy loss, which is the canonical choice for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the label transform from an integer to a set of probabilities\n",
    "def target_transform(inlabel):\n",
    "    newlabel = torch.zeros(10)\n",
    "    newlabel[inlabel] = 1.\n",
    "    return newlabel\n",
    "\n",
    "### Reinitialize datasets with the transforms\n",
    "TrainDS = torchvision.datasets.MNIST(fname, train=True, download=True,\n",
    "            target_transform=target_transform, transform=torchvision.transforms.ToTensor())\n",
    "TestDS = torchvision.datasets.MNIST(fname, train=False,\n",
    "            target_transform=target_transform, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "### Initialize DataLoaders as PyTorch convenience\n",
    "TrainDL = torch.utils.data.DataLoader(TrainDS, shuffle=True, batch_size=32)\n",
    "TestDL = torch.utils.data.DataLoader(TestDS, batch_size=1000)\n",
    "\n",
    "### Choose device: 'cuda' or 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "### Define the dense neuron layer\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),            # 28x28 -> 784\n",
    "    torch.nn.Linear(784, 10),    # 784 -> 10\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af438237",
   "metadata": {},
   "source": [
    "Most of us don't really have an idea what a good value for the binary cross entropy is, so for reference, let's calculate what the loss will be if our model simply estimates for each sample that each digit has an 0.1 probability to be present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline: just say it's anything at probability 1/N, what's the loss?\n",
    "N = 10\n",
    "labels = torch.zeros(1, 10, dtype=torch.float32)\n",
    "labels[0, 3] = 1.\n",
    "output = torch.full_like(labels, 1./N)\n",
    "print(crit(output, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87a005",
   "metadata": {},
   "source": [
    "Now we have a baseline for the loss value.\n",
    "\n",
    "We finally get to training the model. The following block performs 10 training epochs, in each of them loops over all the 32-samples-long batches in the training dataset, repeating the following steps: set the gradient values to zero (these are stored as attributes of the relevant tensors), move the batch samples and labels to the proper device (CPU or CUDA), compute the outputs of the model on that particular batch, compute the loss value, compute the gradients using back-propagation (this is the algorithm that computes the gradients on neural networks) and performs the optimization step. At the end of each epoch, the loss values over all the training samples are averaged and printed along the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736749ca",
   "metadata": {},
   "source": [
    "Now our model is trained. The in-sample (training) loss has decreased considerably, to much less than the uniform probabilities baseline. We need to make sure, however, that the model can generalize and hasn't merely memorized the training dataset. To achieve that, we evaluate the loss on the test dataset. In that process, let's also determine the **accuracy**: the percentage of test samples where the model has assigned the highest probability to the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65a04a",
   "metadata": {},
   "source": [
    "The test loss is close to the training loss. This means that we haven't run into issues with overfitting. The accuracy also seems pretty good - should be around 92%! Let us now see some of the results - out of the following two blocks, the first will show 12 random test examples of digits with their true as well as predicted classes and the confidence level the model has about its prediction, and the second will do the same but only draw from those the model has classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d116381",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw random images from the test dataset and select and show some which have been incorrectly classified\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        while True:\n",
    "            ### Draw the images\n",
    "            test_index = rng.integers(0, len(TestDS))\n",
    "            sample, label = TestDS[test_index]\n",
    "            image, orig_label = TestDS_orig[test_index]\n",
    "            ### Compute the predictions\n",
    "            with torch.no_grad():\n",
    "                output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "                certainty, output = torch.max(output[0], 0)\n",
    "                certainty = certainty.clone().cpu().item()\n",
    "                output = output.clone().cpu().item()\n",
    "                if output!=orig_label:\n",
    "                    break\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81b8ca",
   "metadata": {},
   "source": [
    "## Deep neural networks\n",
    "\n",
    "This is already a simple neural network. However, to build a **deep neural network**, we need to stack several such layers, such that each subsequent layer takes the output of the previous layer as its input. This method of stacking allows us to build very complex models - while the individual structural elements are very simple, the non-linear elements weaved into the network of linear operations allow the model to approximate any function to an arbitrary degree as long as you're flexible on depth and layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show schematic of a fully connected deep neural network\n",
    "import matplotlib.image as mpimg\n",
    "fig, ax = plt.subplots(figsize=(15., 7.))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(mpimg.imread('nn.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47924e38",
   "metadata": {},
   "source": [
    "This network will only have one additional layer to what we've used before. It is again a sequence of PyTorch modules - but there is now an additional layer. The first layer gives 50 outputs with the $\\mathrm{ReLU}\\left(x\\right) = \\mathrm{max}\\left(0, x\\right)$ activation function and the second gives 10 outputs with the $\\mathrm{Softmax}$ activation. The rest of the code in this section works the same way as with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a simple two-layer network\n",
    "Network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50, 10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6079df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f621710",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5a0cf",
   "metadata": {},
   "source": [
    "This model has about 4 times as many trainable parameters as the previous one, which brings an increased risk of overfitting. However, the test loss indicates that we haven't run into overfitting issues yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10980d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw random images from the test dataset and select and show some which have been incorrectly classified\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        while True:\n",
    "            ### Draw the images\n",
    "            test_index = rng.integers(0, len(TestDS))\n",
    "            sample, label = TestDS[test_index]\n",
    "            image, orig_label = TestDS_orig[test_index]\n",
    "            ### Compute the predictions\n",
    "            with torch.no_grad():\n",
    "                output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "                certainty, output = torch.max(output[0], 0)\n",
    "                certainty = certainty.clone().cpu().item()\n",
    "                output = output.clone().cpu().item()\n",
    "                if output!=orig_label:\n",
    "                    break\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160517a",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "A slightly more convoluted (sorry for the bad pun) type of neural networks has been inspired by the animal visual cortex. Convolutional layers are introduced, which slide *filters* over an input image, producing another image - often with a larger number of channels, so these can be hard to visualize. Each pixel in that output image is then basically the product of a smaller fully connected layer (like we used before) applied to a small portion of the input image. These layers are used to build **convolutional neural networks**, which typically consist of a convolutional part and a dense part. The convolutional part is constructed out of alternating convolutional layers and pooling layers (these downsample the image between the convolutional layers), the result is flattened and passed to a dense part, which is typically a deep neural network as shown before.\n",
    "\n",
    "This structure removes some less crucial connections between neurons and use many weights for multiple connections as opposed to a fully connected network. This allows us to build a network just as suited to some problems as a fully connected network while dramatically reducing the number of free parameters to be optimized. They have proven to be particularly suitable to computer vision problems.\n",
    "\n",
    "Go ahead and play with the following - it's the same code as before, just a different network design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d149fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15., 7.))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(mpimg.imread('cnn.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a simple convolutional neural network\n",
    "Network = torch.nn.Sequential(      #  1x28x28\n",
    "    torch.nn.Conv2d(1, 12, (9, 9)),  #  12x20x20\n",
    "    torch.nn.MaxPool2d((2, 2)),     #  12x10x10\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(12, 24, (5, 5)), # 24x 6x 6\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((3, 3)),     # 24x 2x 2\n",
    "    torch.nn.Flatten(),             #       96\n",
    "    torch.nn.Linear(96, 16),        #       16\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 10),        #       10\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "Network.to(device=device)\n",
    "\n",
    "### Get information about model\n",
    "totpars = 0\n",
    "for par in Network.parameters():\n",
    "    newpars = 1\n",
    "    for num in par.shape:\n",
    "        newpars *= num\n",
    "    totpars += newpars\n",
    "print(Network)\n",
    "print('%i trainable parameters' % totpars)\n",
    "\n",
    "### Initialize loss function and optimizer\n",
    "opt = torch.optim.SGD(Network.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ae9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Set model in training mode and create the epochs axis\n",
    "Network.train()\n",
    "epochs = range(1, 11)\n",
    "\n",
    "### Train the model\n",
    "for e in epochs:\n",
    "    tr_loss = 0.\n",
    "    samples = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TrainDL:\n",
    "        opt.zero_grad() # zero gradient values\n",
    "        inputs = inputs.to(device=device) # move input and label tensors to the device with the model\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs) # compute model outputs\n",
    "        loss = crit(outputs, labels) # compute batch loss\n",
    "        loss.backward() # back-propagate the gradients\n",
    "        opt.step() # update the model weights\n",
    "        tr_loss += loss.clone().cpu().item()*len(inputs) # add the batch loss to the running loss\n",
    "        samples += len(inputs) # update the number of processed samples\n",
    "    tr_loss /= samples # compute training loss\n",
    "    print(e, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set model in evaluation mode\n",
    "Network.eval()\n",
    "\n",
    "### Compute the test loss\n",
    "with torch.no_grad():\n",
    "    te_loss = 0.\n",
    "    samples = 0\n",
    "    accuracy = 0\n",
    "    ### Loop over batches\n",
    "    for inputs, labels in TestDL:\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = Network(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        te_loss += loss.clone().cpu().item()*len(inputs)\n",
    "        accuracy += torch.sum(torch.eq(torch.max(labels, 1)[1], torch.max(outputs, 1)[1]), dtype=int).clone().cpu().item()\n",
    "        samples += len(inputs)\n",
    "    te_loss /= samples\n",
    "    accuracy /= samples\n",
    "    print('Test loss: %f, accuracy: %f' % (te_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b659e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw some random images from the test dataset and compare the true labels to the network outputs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        ### Draw the images\n",
    "        test_index = rng.integers(0, len(TestDS))\n",
    "        sample, label = TestDS[test_index]\n",
    "        image, orig_label = TestDS_orig[test_index]\n",
    "        ### Compute the predictions\n",
    "        with torch.no_grad():\n",
    "            output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "            certainty, output = torch.max(output[0], 0)\n",
    "            certainty = certainty.clone().cpu().item()\n",
    "            output = output.clone().cpu().item()\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ce896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Draw random images from the test dataset and select and show some which have been incorrectly classified\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(15., 6.))\n",
    "### Loop over subplots\n",
    "for axes_row in axes:\n",
    "    for ax in axes_row:\n",
    "        while True:\n",
    "            ### Draw the images\n",
    "            test_index = rng.integers(0, len(TestDS))\n",
    "            sample, label = TestDS[test_index]\n",
    "            image, orig_label = TestDS_orig[test_index]\n",
    "            ### Compute the predictions\n",
    "            with torch.no_grad():\n",
    "                output = Network(torch.unsqueeze(sample, dim=0).to(device=device))\n",
    "                certainty, output = torch.max(output[0], 0)\n",
    "                certainty = certainty.clone().cpu().item()\n",
    "                output = output.clone().cpu().item()\n",
    "                if output!=orig_label:\n",
    "                    break\n",
    "        ### Show image\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title('True: %i, predicted: %i\\nat %f' % (orig_label, output, certainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8b020",
   "metadata": {},
   "source": [
    "Congratulations! You have managed to get to the point where your network mostly misclassifies only those digits, which are badly written and are outliers in the dataset. We've only scratched the surface of what you can do with neural networks, but you've learned the most elementary things you need to know. Go ahead and play with this notebook - with the networks, with the training parameters, etc. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
